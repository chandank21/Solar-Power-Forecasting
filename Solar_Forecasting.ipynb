{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score,mean_squared_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    raw_df = []\n",
    "    dict_df = pd.read_excel(r'C:\\Users\\Chandan\\Desktop\\Analog_Reports\\Solar data\\GridTieInverterData.xlsx',skiprows=5,parse_dates={'Time':[2]},index_col='Time',skipfooter=1,sheet_name=None)\n",
    "    for key,dframe in dict_df.items():\n",
    "        dframe.drop(dframe.tail(1).index,inplace=True)\n",
    "        raw_df.append(dframe[['PDC(W)']])\n",
    "    raw_df=pd.concat(raw_df)\n",
    "    raw_df.index=pd.to_datetime(raw_df.index,format='%Y-%m-%d %H:%M:%S')\n",
    "    raw_df.sort_index(inplace=True,ascending=False)\n",
    "    return raw_df\n",
    "\n",
    "def load_weather():\n",
    "    weather = pd.read_csv(r'C:\\Users\\Chandan\\Desktop\\Analog_Reports\\Solar data\\kanpur_weather_solar.csv',\n",
    "                             header=25)\n",
    "    LIST=[]\n",
    "    for i in range(weather.shape[0]):\n",
    "        if weather.HR.values[i] < 10:\n",
    "            string = f\"{weather.YEAR.values[i]}-{weather.MO.values[i]}-{weather.DY.values[i]} {0}{weather.HR.values[i]}\"\n",
    "        else:\n",
    "            string = f\"{weather.YEAR.values[i]}-{weather.MO.values[i]}-{weather.DY.values[i]} {weather.HR.values[i]}\"\n",
    "\n",
    "        LIST.append(pd.to_datetime(string,format='%Y-%m-%d %H'))\n",
    "\n",
    "    weather['time']=LIST\n",
    "    weather=weather.set_index(weather['time'],drop=False)\n",
    "    weather.drop(['YEAR','MO','DY','HR','time'],axis=1,inplace=True)\n",
    "    return weather\n",
    "\n",
    "def clean_data(df):\n",
    "    df['datetime'] = [str(index)[0:13] for index in df.index.values]\n",
    "    df = df.groupby('datetime').mean()\n",
    "    df.index=pd.to_datetime(df.index,format='%Y-%m-%d %H')\n",
    "    return df\n",
    "\n",
    "def fill_zero_pow(df,period):\n",
    "    start,end=df.index[0],df.index[-1]\n",
    "    index=pd.date_range(start=start,end=end,freq='1H')\n",
    "    t_df=pd.DataFrame({'val':np.NaN},index=index)\n",
    "    result = df.join(t_df, how=\"outer\").drop(['val'],axis=1)\n",
    "    for index in result.index:\n",
    "        if index.hour in period:\n",
    "            result.loc[index]=0\n",
    "    return result\n",
    "\n",
    "def scaling(df):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(df)\n",
    "    return scaler\n",
    "\n",
    "def add_weather(df):\n",
    "    weather_df=load_weather()\n",
    "    combine_df = pd.merge(df, weather_df, left_index=True, right_index=True)\n",
    "    return combine_df,weather_df\n",
    "\n",
    "def apply_pca(X):\n",
    "    pca = PCA(0.95)\n",
    "    pca = pca.fit(X)\n",
    "    return pca\n",
    "\n",
    "def create_model(train_input, train_target, model_config):\n",
    "    model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "    model.fit(train_input, train_target.ravel())\n",
    "    return model\n",
    "\n",
    "def fill_missing(df):\n",
    "    missing_index=df[df['PDC(W)'].isna()].index.to_list()\n",
    "    dff = df.dropna(axis=0,inplace = False)\n",
    "    combine_df,weather_df = add_weather(dff)\n",
    "    \n",
    "    target = ['PDC(W)']\n",
    "    input_data = combine_df.drop(columns=target,axis=1,inplace=False).values\n",
    "    target = combine_df[target].values\n",
    "    \n",
    "    scaler = scaling(input_data)\n",
    "    scaled = scaler.transform(input_data)\n",
    "    \n",
    "    pca= apply_pca(scaled)\n",
    "    pca_input = pca.transform(scaled)\n",
    "    \n",
    "    train_input,test_input,train_target,test_target = train_test_split(pca_input,target,test_size=0.1)\n",
    "    model_config = []\n",
    "    model = create_model(train_input,train_target,model_config=model_config)\n",
    "    \n",
    "    # predicting missing....\n",
    "    missing_inp = weather_df.loc[missing_index].values\n",
    "    missing_inp = pca.transform(scaler.transform(missing_inp))\n",
    "    missing_val = model.predict(missing_inp)\n",
    "    df.loc[missing_index] = missing_val.reshape(-1,1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadind data....\n",
    "raw_df= load_data()\n",
    "# making time period hourly wise.....\n",
    "clean_df = clean_data(raw_df)\n",
    "#filling black_period.......\n",
    "zero_pow_period = [19,20,21,22,23,0,1,2,3,4]\n",
    "result = fill_zero_pow(clean_df,period=zero_pow_period)\n",
    "df_filled = fill_missing(result)\n",
    "combine_df,_ = add_weather(df_filled)\n",
    "\n",
    "target = ['PDC(W)']\n",
    "input_data = combine_df.drop(columns=target,axis=1,inplace=False).values\n",
    "target = combine_df[target].values\n",
    "\n",
    "# dimention reduction.....\n",
    "scaler = scaling(input_data)\n",
    "scaled = scaler.transform(input_data)\n",
    "\n",
    "pca= apply_pca(scaled)\n",
    "pca_input = pca.transform(scaled)\n",
    "\n",
    "train_input,test_input,train_target,test_target = train_test_split(pca_input,target,test_size=0.3)\n",
    "model_config = []\n",
    "model = create_model(train_input,train_target,model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set model score....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9784516130284252"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = model.predict(test_input)\n",
    "train_result = model.predict(train_input)\n",
    "print(f\"Training set model score....\")\n",
    "model.score(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set model score....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8281603169306806"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Test set model score....\")\n",
    "model.score(test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test...summary\n",
      "explained_variance_score = 0.7970311236327194\n",
      "mean_squared_error = 206482.12050862072\n",
      "mean_absolute_percentage_error = 685069323993029.6\n"
     ]
    }
   ],
   "source": [
    "def print_summary():\n",
    "    print(f\"test...summary\")\n",
    "    print(f\"explained_variance_score = {explained_variance_score(test_result,test_target)}\")\n",
    "    print(f\"mean_squared_error = {mean_squared_error(test_result,test_target)}\")\n",
    "    print(f\"mean_absolute_percentage_error = {mean_absolute_percentage_error(test_result,test_target)}\")\n",
    "print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(df_as_np,n_lags,n_out=1):\n",
    "    X,y = [],[]\n",
    "    for i in range(len(df_as_np)-n_lags):\n",
    "        row = [ a for a in df_as_np[i:i+n_lags]]\n",
    "        X.append(row)\n",
    "        label = df_as_np[i+n_lags:i+n_lags+n_out,0]\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
